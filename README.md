# Engram AI üß†

**Neuroscience-grounded memory system for AI agents with semantic search and auto-fallback**

[![License](https://img.shields.io/badge/license-MIT-blue)](LICENSE)
[![Python](https://img.shields.io/badge/python-3.9+-blue)](https://www.python.org/)

## What is Engram?

Engram is a production-ready memory system for AI agents, inspired by cognitive neuroscience. It provides:

- üß† **ACT-R Activation** - Memory recall based on frequency, recency, and importance
- üîó **Hebbian Learning** - Automatic association between co-activated memories  
- üí§ **Consolidation** - Transfer memories from working ‚Üí long-term storage
- üåç **Semantic Search** - Cross-language memory recall (50+ languages)
- üîÑ **Auto-Fallback** - Zero-config deployment with automatic provider detection

## Quick Start

### Installation

```bash
# Basic installation (FTS5 keyword search only)
pip install engramai

# With semantic search (recommended)
pip install "engramai[sentence-transformers]"

# With all embedding providers
pip install "engramai[all]"
```

### Basic Usage

```python
from engram import Memory

# Create memory system (auto-detects best embedding provider)
memory = Memory("./my-agent.db")

# Store memories
memory.add("User prefers detailed explanations", type="relational", importance=0.8)
memory.add("Project deadline: Feb 10", type="factual")

# Recall memories (semantic search)
results = memory.recall("user preferences", limit=5)
for r in results:
    print(f"{r['confidence']:.2f}: {r['content']}")

# Run consolidation (strengthens important memories)
memory.consolidate(days=1.0)
```

### MCP Server (for OpenClaw, BotCore, etc.)

```bash
# Set database path
export ENGRAM_DB_PATH=./my-agent.db

# Start MCP server (auto-detects embedding provider)
python3 -m engram.mcp_server

# Or configure specific provider
export ENGRAM_EMBEDDING=sentence-transformers  # or ollama, openai, none, auto
python3 -m engram.mcp_server
```

## Features

### üéØ Zero-Config Deployment

Engram automatically detects and uses the best available embedding provider:

1. **Ollama** (if running locally with embedding models)
2. **Sentence Transformers** (if installed)
3. **OpenAI** (if API key configured)
4. **FTS5** (always available as fallback)

```bash
# Just install and go - no configuration needed!
pip install "engramai[sentence-transformers]"
python3 -m engram.mcp_server
```

### üåç Cross-Language Semantic Search

Find memories across languages with zero additional configuration:

```python
memory.add("marketingÊòØ‰∏™Â§ßÈöæÈ¢ò")  # Chinese

# Query in English - finds the Chinese memory!
results = memory.recall("marketing is difficult")
# ‚úÖ Returns: "marketingÊòØ‰∏™Â§ßÈöæÈ¢ò"
```

Supports 50+ languages including:
- English, Chinese, Spanish, French, German, Russian
- Japanese, Korean, Arabic, Hindi
- And many more...

### üî¨ Neuroscience-Grounded

Based on cognitive science models:

- **ACT-R** - Activation from frequency, recency, spreading activation
- **Hebbian Learning** - "Neurons that fire together, wire together"
- **Memory Consolidation** - Simulates sleep-based memory strengthening
- **Forgetting Curve** - Natural decay based on Ebbinghaus' research

### üìä Session-Aware Working Memory

Reduces API calls by 70-80% for continuous conversations:

```python
# First query - full retrieval
results = memory.session_recall("user preferences", session_id="chat_123")

# Follow-up query on same topic - uses cached working memory!
results = memory.session_recall("what does user like?", session_id="chat_123")
# ‚ö° No database query - instant response
```

## ‚ö° Performance

**Real production data from OpenClaw deployment (Feb 2026):**

### Token Consumption: ‚âà **$0** üéâ

- **Theoretical cost:** +250-500 tokens/turn from memory injection
- **Actual cost:** **$0** (absorbed by prompt caching)
- Anthropic caches entire system prompt including injected memories
- Cache Read tokens: 87,726+ per session (massive hits)

### Response Latency: **No perceptible slowdown** üöÄ

- **Recall overhead:** ~90ms (async, doesn't block LLM)
- **Store overhead:** ~50ms (fire-and-forget)
- **Real timing:** 4-8 seconds per turn (normal, unchanged)
- **Smart filtering:** Skips ~50% of messages (greetings, heartbeats)

### Optimization Mechanisms

1. ‚úÖ **Smart filtering** - 50% message reduction
2. ‚úÖ **Prompt caching** - Zero token overhead
3. ‚úÖ **Graceful failure** - Errors don't break bot
4. ‚úÖ **MCP connection pooling** - 4x latency reduction

**Conclusion:** Infinite context with zero performance cost.

üìä **[Full Performance Analysis ‚Üí](PERFORMANCE.md)**

---

## Configuration

### Environment Variables

| Variable | Values | Description |
|----------|--------|-------------|
| `ENGRAM_EMBEDDING` | `auto` (default), `sentence-transformers`, `ollama`, `openai`, `none` | Embedding provider |
| `ENGRAM_ST_MODEL` | Model name (default: `paraphrase-multilingual-MiniLM-L12-v2`) | Sentence Transformers model |
| `ENGRAM_OLLAMA_MODEL` | Model name (default: `nomic-embed-text`) | Ollama embedding model |
| `OPENAI_API_KEY` | API key | Required for OpenAI embeddings |
| `ENGRAM_DB_PATH` | File path | Database location (for MCP server) |

### Provider Comparison

| Provider | Pros | Cons | Use When |
|----------|------|------|----------|
| **Auto** ‚≠ê (default) | Zero config, adapts to environment | Non-deterministic selection | Production, distribution |
| **Sentence Transformers** | Free, offline, 50+ languages | ~118MB model download | Privacy-sensitive, no API costs |
| **Ollama** | Free, offline, customizable | Requires Ollama running | You already use Ollama |
| **OpenAI** | Highest quality | Costs API credits, needs internet | Prototyping, cloud-only |
| **None (FTS5)** | No dependencies, instant | Keyword-only, no semantic search | Testing, minimal setups |

## Examples

### Store Different Memory Types

```python
# Factual knowledge
memory.add("Paris is the capital of France", type="factual")

# Personal relationships
memory.add("User likes detailed technical explanations", type="relational", importance=0.9)

# Procedural knowledge (how-to)
memory.add("To deploy: git push origin main", type="procedural", importance=0.8)

# Episodic memories (events)
memory.add("Shipped feature X on Jan 15", type="episodic")
```

### Recall with Filters

```python
# Only relational memories
results = memory.recall("user preferences", types=["relational"], limit=3)

# High-confidence only
results = memory.recall("deadlines", min_confidence=0.7)

# Context-aware (spreading activation)
results = memory.recall("project status", context=["planning", "timeline"])
```

### Memory Consolidation

```python
# Simulate one day of sleep (strengthens important memories)
memory.consolidate(days=1.0)

# Prune weak memories below threshold
memory.forget(threshold=0.01)

# Apply reward/punishment
memory.reward("Great job!", recent_n=3)  # Strengthens last 3 memories
```

### Export/Import

```python
# Export to file
memory.export("backup.db")

# Import from file (Python API)
from shutil import copyfile
copyfile("backup.db", "./my-agent.db")
memory = Memory("./my-agent.db")
```

## Architecture

```
User Query
    ‚Üì
Vector Search (semantic)
    ‚Üì
FTS5 Search (lexical)
    ‚Üì
Merge & Dedupe
    ‚Üì
ACT-R Activation (cognitive dynamics)
    ‚Üì
Confidence Scoring (metacognition)
    ‚Üì
Ranked Results
```

## Development

```bash
# Clone repository
git clone https://github.com/tonitangpotato/engramai.git
cd engramai

# Install in development mode
pip install -e ".[dev,all]"

# Run tests
pytest

# Run provider detection test
python3 engram/provider_detection.py
```

## Integration

### With OpenClaw

Engram is the default memory system for OpenClaw agents.

### With BotCore

```typescript
import { createBot } from 'botcore';

const bot = await createBot({ workspace: './my-bot' });
await bot.memory.store('Important context', { type: 'factual' });
const results = await bot.memory.recall('context');
```

### Standalone Python

```python
from engram import Memory

memory = Memory("./agent.db")
memory.add("Remember this", importance=0.8)
results = memory.recall("what to remember?")
```

### MCP Protocol

Any MCP-compatible client can use Engram:

```json
{
  "mcpServers": {
    "engram": {
      "command": "python3",
      "args": ["-m", "engram.mcp_server"],
      "env": {
        "ENGRAM_DB_PATH": "./my-agent.db"
      }
    }
  }
}
```

## Performance

| Metric | Value | Notes |
|--------|-------|-------|
| Model size | 118MB | One-time download (Sentence Transformers) |
| Startup time | ~200ms | After first download |
| Vector generation | ~250 mem/sec | CPU (M2 chip) |
| Search latency | 10-50ms | 1000 memories |
| Cross-language accuracy | 100% | Test cases: 3/3 ‚úÖ |

## Documentation

- [Embedding Configuration Guide](engram/EMBEDDING-CONFIG.md)
- [Phase 1-2 Summary](PHASE1-2-SUMMARY.md)
- [Phase 3 Complete Report](PHASE3-COMPLETE.md)
- [API Documentation](docs/API.md)

## Credits

Engram is inspired by:
- **ACT-R** (Adaptive Control of Thought-Rational) - Carnegie Mellon
- **Hebbian Learning** - Donald Hebb
- **Memory Consolidation** - Sleep research by Walker, Stickgold
- **Forgetting Curve** - Hermann Ebbinghaus

## License

MIT License - see [LICENSE](LICENSE) for details.

## Support

- Issues: https://github.com/tonitangpotato/engramai/issues
- Discussions: https://github.com/tonitangpotato/engramai/discussions
- OpenClaw Discord: https://discord.com/invite/clawd
