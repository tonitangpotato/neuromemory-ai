\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{natbib}
\usepackage{xcolor}

% NeurIPS style margins (approximate)
\usepackage[margin=1in]{geometry}

\title{Beyond Vector Search: Cognitive Memory Dynamics for Language Model Agents}

\author{
  Anonymous Authors \\
  \texttt{anonymous@example.com}
}

\date{}

\begin{document}

\maketitle

\begin{abstract}
Memory systems for AI agents have converged on a single paradigm: embedding text as vectors and retrieving by cosine similarity. This approach treats memory as static information retrieval, ignoring decades of cognitive science research on how biological memory actually works. We present \textsc{NeuromemoryAI}, a memory system that implements established models from cognitive psychology: ACT-R activation dynamics for principled retrieval, the Memory Chain Model for working-to-long-term consolidation, Ebbinghaus forgetting curves for adaptive decay, and Hebbian learning for emergent associative connections. Our key insight is that large language models already provide semantic understanding---they excel at interpreting meaning from text. What they lack is principled \emph{memory dynamics}: knowing when to surface information based on context and history, what to deprioritize as it becomes stale, and how knowledge should evolve through use. We introduce Hebbian learning for emergent memory associations without requiring manual entity tagging or named entity recognition. The system is implemented in pure Python with zero external dependencies, relying only on the standard library and SQLite for storage. Experiments on multi-session agent tasks show that cognitive-grounded memory improves retrieval relevance by [TODO]\% compared to vector-based approaches (Mem0, Zep) while requiring no embedding computation.
\end{abstract}

\section{Introduction}

The rise of large language model (LLM) agents has created urgent demand for persistent memory systems. Agents built on frameworks like LangChain, AutoGPT, and CrewAI need to remember user preferences, past conversations, learned facts, and ongoing task context across sessions. Without memory, each interaction starts from zero---an assistant that cannot recall your name, your projects, or what you discussed yesterday.

The dominant solution has been vector databases. Systems like Mem0, Zep, and Pinecone embed text into high-dimensional vectors using models like OpenAI's text-embedding-ada or open-source alternatives. Retrieval becomes nearest-neighbor search in embedding space. This approach has clear merits: semantic similarity captures meaning beyond keyword matching, and vector databases scale efficiently.

However, this paradigm treats memory as \emph{static information retrieval}. A memory's relevance is determined solely by its semantic similarity to the current query. There is no notion of:
\begin{itemize}
    \item \textbf{Temporal dynamics}: A memory accessed yesterday should be more available than one untouched for months
    \item \textbf{Contextual spreading}: Recalling ``machine learning'' should prime related concepts like ``neural networks'' and ``gradient descent''
    \item \textbf{Consolidation}: Recent episodic experiences should gradually become stable semantic knowledge
    \item \textbf{Adaptive forgetting}: Irrelevant or outdated information should fade, improving signal-to-noise ratio
\end{itemize}

These are not speculative features---they are established phenomena in cognitive psychology, formalized in models like ACT-R \citep{anderson2007}, the Memory Chain Model \citep{murre2011}, and Hebbian learning \citep{hebb1949}. Biological memory is not a static database; it is a dynamic system that strengthens with use, fades without it, and continuously reorganizes based on experience.

\subsection{The Key Insight}

Our central observation is that LLMs already provide semantic understanding. When you embed text using a language model, you are essentially asking the model to encode meaning. But if an LLM is already present in the agent pipeline---which it almost always is---this embedding step is redundant. The LLM can directly interpret retrieved text and determine relevance.

What the LLM \emph{cannot} provide is memory dynamics. It has no mechanism to track that a particular memory was accessed three times last week and should therefore be more readily available. It cannot consolidate recent experiences into stable knowledge. It has no principled way to forget.

This motivates our approach: use the LLM for semantics, use cognitive models for dynamics. \textsc{NeuromemoryAI} implements:
\begin{enumerate}
    \item \textbf{ACT-R activation}: Memories gain activation through recency and frequency of access, with spreading activation from current context
    \item \textbf{Hebbian learning}: Memories co-activated during recall automatically form associative links, enabling emergent structure without NER
    \item \textbf{Memory Chain consolidation}: Two-trace model with fast-decaying working memory and stable long-term storage
    \item \textbf{Ebbinghaus forgetting}: Exponential decay with stability growth through retrieval, implementing spaced repetition effects
\end{enumerate}

\subsection{Contributions}

\begin{enumerate}
    \item We present the first implementation of the ACT-R activation model for AI agent memory, providing mathematically grounded retrieval scoring based on access patterns
    \item We introduce Hebbian learning for emergent memory associations, eliminating the need for named entity recognition or manual tagging
    \item We implement Memory Chain Model consolidation, enabling dual-trace dynamics between working and long-term memory
    \item We release \textsc{NeuromemoryAI} as open-source (Python + TypeScript), implemented with zero external dependencies
    \item We provide benchmarks against Mem0, Zep, and shodh-memory on multi-session agent tasks
\end{enumerate}

\section{Background and Related Work}

\subsection{Cognitive Science Models of Memory}

\paragraph{ACT-R (Adaptive Control of Thought---Rational)}
The ACT-R architecture \citep{anderson2007} models human cognition, with memory retrieval governed by activation. A memory chunk's activation $A_i$ determines its probability of retrieval:
\begin{equation}
A_i = B_i + \sum_j W_j S_{ji} + \epsilon
\end{equation}
where $B_i$ is base-level activation (reflecting recency and frequency), $W_j S_{ji}$ is spreading activation from context elements, and $\epsilon$ is noise. Base-level activation follows:
\begin{equation}
B_i = \ln \left( \sum_{k=1}^{n} t_k^{-d} \right)
\label{eq:baselevel}
\end{equation}
where $t_k$ is the time since the $k$-th access and $d \approx 0.5$ is the decay parameter. This captures the power-law of forgetting observed empirically.

\paragraph{Memory Chain Model}
\citet{murre2011} proposed the Memory Chain Model (MCM) to explain consolidation dynamics. Memory exists in two traces:
\begin{align}
\frac{dr_1}{dt} &= -\mu_1 r_1 \label{eq:mcm1}\\
\frac{dr_2}{dt} &= \alpha r_1 - \mu_2 r_2 \label{eq:mcm2}
\end{align}
where $r_1$ is the fast-decaying working memory trace, $r_2$ is the slow-decaying long-term trace, $\mu_1 > \mu_2$ are decay rates, and $\alpha$ is the consolidation rate. This explains why recent memories are vivid but fragile, while old memories are stable but less detailed.

\paragraph{Ebbinghaus Forgetting Curves}
\citet{ebbinghaus1885} established that forgetting follows an exponential decay:
\begin{equation}
R(t) = e^{-t/S}
\label{eq:forgetting}
\end{equation}
where $R$ is retrievability and $S$ is stability. Crucially, each successful retrieval increases stability, implementing the spacing effect: distributed practice leads to better retention than massed practice.

\paragraph{Hebbian Learning}
\citet{hebb1949} proposed that ``neurons that fire together wire together''---simultaneous activation strengthens connections. In memory terms, concepts frequently co-activated become associated. We formalize this as:
\begin{equation}
\Delta w_{ij} = \eta \cdot a_i \cdot a_j
\label{eq:hebbian}
\end{equation}
where $w_{ij}$ is the associative strength between memories $i$ and $j$, $a_i, a_j$ are their activations, and $\eta$ is the learning rate.

\subsection{AI Memory Systems}

\paragraph{Mem0} Provides vector-based memory with manual management APIs. Users explicitly add, search, and delete memories. Retrieval uses embedding similarity with optional filtering.

\paragraph{Zep} Extends vector search with temporal awareness, allowing queries like ``memories from last week.'' Still fundamentally embedding-based retrieval.

\paragraph{shodh-memory} A Rust-based system implementing Hebbian learning with bundled TinyBERT for named entity recognition. Designed for edge deployment. The key difference from our approach: shodh requires NER to identify entities for linking, while we use co-activation to form links organically.

\paragraph{HippoRAG} \citet{yu2024hipporag} propose hippocampal-inspired retrieval augmentation, modeling the hippocampus's role in binding disparate cortical representations. Focuses on retrieval augmentation rather than persistent agent memory.

\paragraph{LangChain Memory} Provides simple patterns: conversation buffer, summary memory, entity memory. These are engineering heuristics without cognitive grounding.

\subsection{Gap in Literature}

Existing AI memory systems either:
\begin{enumerate}
    \item Use embedding similarity as the sole relevance signal (Mem0, Zep, Pinecone)
    \item Require NER/entity extraction for structure (shodh-memory)
    \item Lack principled forgetting and consolidation (all current systems)
\end{enumerate}

No system implements the full suite of cognitive dynamics: activation-based retrieval, Hebbian association, consolidation, and forgetting. \textsc{NeuromemoryAI} fills this gap.

\section{System Design}

\subsection{Architecture Overview}

\textsc{NeuromemoryAI} positions itself between the LLM and storage, providing memory dynamics while delegating semantic understanding to the LLM:

\begin{figure}[h]
\centering
\begin{verbatim}
┌─────────────────┐
│  LLM (external) │  ← Semantic understanding
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│ NeuromemoryAI   │  ← Memory dynamics
│  ├── ACT-R      │     (activation, retrieval)
│  ├── Hebbian    │     (association learning)
│  ├── Forgetting │     (decay, stability)
│  └── Consolidate│     (working→long-term)
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│  SQLite + FTS5  │  ← Storage + full-text search
└─────────────────┘
\end{verbatim}
\caption{System architecture. The LLM handles semantic interpretation; NeuromemoryAI handles memory dynamics.}
\end{figure}

\subsection{ACT-R Activation Model}

Each memory has an activation score computed at retrieval time:

\begin{equation}
A_i = \underbrace{\ln \left( \sum_{k=1}^{n} t_k^{-d} \right)}_{\text{base-level}} + \underbrace{\sum_{j \in C} W_j S_{ji}}_{\text{spreading}} + \underbrace{\gamma \cdot I_i}_{\text{importance}}
\end{equation}

\paragraph{Base-level activation} reflects access history. Recent accesses contribute more ($t_k^{-d}$ where $d=0.5$). Frequent access accumulates through the sum. This naturally implements recency and frequency effects.

\paragraph{Spreading activation} propagates from current context. If the user mentions ``Python,'' memories about Python programming receive activation boost. We implement this through Hebbian links: if memory $j$ is in context and has a link to memory $i$, activation spreads proportionally to link strength.

\paragraph{Importance modulation} captures emotional/motivational salience---an amygdala analog. Memories marked important (high $I_i$) receive a boost, modeling how emotionally significant events are better remembered.

\paragraph{Retrieval probability} follows the softmax:
\begin{equation}
P(\text{retrieve } i) = \frac{e^{A_i / \tau}}{\sum_j e^{A_j / \tau}}
\end{equation}
In practice, we retrieve the top-$k$ by activation rather than sampling.

\subsection{Hebbian Learning for Emergent Associations}

When memories are co-activated during recall, we strengthen their connection:

\begin{algorithm}
\caption{Hebbian Link Formation}
\begin{algorithmic}
\STATE \textbf{Input:} Retrieved memories $M = \{m_1, ..., m_k\}$
\FOR{each pair $(m_i, m_j)$ in $M$}
    \STATE $\text{coactivation}[m_i, m_j] \gets \text{coactivation}[m_i, m_j] + 1$
    \IF{$\text{coactivation}[m_i, m_j] \geq \theta$}
        \STATE Create or strengthen link $(m_i, m_j)$
    \ENDIF
\ENDFOR
\end{algorithmic}
\end{algorithm}

With threshold $\theta = 3$, links form after three co-retrievals. This replaces NER-based entity linking:
\begin{itemize}
    \item NER approach: Extract ``Python'' and ``machine learning'' as entities, manually link
    \item Hebbian approach: User frequently asks about Python ML projects → memories naturally link
\end{itemize}

The Hebbian approach captures \emph{usage patterns} rather than \emph{surface entities}. Two memories about the same project might not share entity names but will link through co-activation.

\subsection{Memory Chain Consolidation}

We implement dual-trace dynamics following Equations \ref{eq:mcm1}--\ref{eq:mcm2}:

\paragraph{Working memory} ($r_1$): Fast decay ($\mu_1 = 0.1$), stores recent episodic traces. High initial activation, quickly fades.

\paragraph{Long-term memory} ($r_2$): Slow decay ($\mu_2 = 0.01$), stores consolidated knowledge. Lower initial activation, stable over time.

\paragraph{Consolidation} transfers activation from $r_1$ to $r_2$ at rate $\alpha = 0.05$ per consolidation cycle. We trigger consolidation during ``sleep'' phases---agent idle time or explicit consolidate calls.

\paragraph{Interleaved replay}: During consolidation, we replay recent memories alongside older ones. This prevents catastrophic forgetting and strengthens cross-temporal associations. The replay distribution samples:
\begin{itemize}
    \item 50\% from recent (last 24h)
    \item 30\% from medium-term (1-7 days)
    \item 20\% from long-term (older)
\end{itemize}

\subsection{Ebbinghaus Forgetting with Stability}

Each memory has a \emph{stability} $S$ that grows with successful retrieval:
\begin{equation}
S' = S \cdot (1 + \beta)
\end{equation}
where $\beta \approx 0.1$. Retrievability decays as Equation \ref{eq:forgetting}. High-stability memories decay slowly; low-stability memories fade quickly.

This implements spaced repetition effects: memories retrieved at increasing intervals develop high stability. Memories crammed in one session but never revisited fade rapidly.

\paragraph{Memory-type specific decay:}
\begin{itemize}
    \item Episodic (events): Fast decay, $S_0 = 1.0$
    \item Semantic (facts): Slow decay, $S_0 = 5.0$
    \item Procedural (how-to): Very slow decay, $S_0 = 10.0$
\end{itemize}

\subsection{Additional Mechanisms}

\paragraph{Synaptic homeostasis} implements global downscaling. When total memory activation exceeds threshold, all activations are scaled down proportionally:
\begin{equation}
A_i' = A_i \cdot \frac{\tau}{\sum_j A_j}
\end{equation}
This prevents activation explosion and models sleep-dependent synaptic renormalization \citep{tononi2006}.

\paragraph{Contradiction detection} identifies conflicting memories. When adding a new memory, we check for semantic opposition and add \texttt{contradicts}/\texttt{contradicted\_by} links with a 0.3× confidence penalty.

\paragraph{Two-dimensional confidence} separates:
\begin{itemize}
    \item \textbf{Reliability}: How trustworthy is the source?
    \item \textbf{Salience}: How important/relevant is this memory?
\end{itemize}

\section{Implementation}

\subsection{Zero External Dependencies}

\textsc{NeuromemoryAI} uses only Python standard library and SQLite (included in Python). No numpy, no torch, no API calls. This design choice maximizes portability:
\begin{itemize}
    \item Works in any Python environment
    \item No version conflicts
    \item No network requirements
    \item Minimal attack surface
\end{itemize}

The core implementation is approximately 500 lines of code.

\subsection{Storage: SQLite + FTS5}

We use SQLite's FTS5 extension for full-text search. While less flexible than embeddings for semantic similarity, FTS5 provides:
\begin{itemize}
    \item Zero latency (no API calls)
    \item Exact phrase matching
    \item Boolean operators (AND, OR, NOT)
    \item Proximity search
    \item BM25 ranking
\end{itemize}

For keyword-based recall augmented by LLM semantic interpretation, this is often sufficient.

\subsection{Configuration Presets}

We provide four presets optimized for different agent types:

\begin{table}[h]
\centering
\begin{tabular}{lcccc}
\toprule
Preset & Decay & Replay & Consolidation & Focus \\
\midrule
Chatbot & Slow & High & Frequent & Relationship \\
Task Agent & Fast & Low & Rare & Procedural \\
Personal Assistant & Medium & Medium & Daily & Balanced \\
Researcher & Very slow & High & Weekly & Archive \\
\bottomrule
\end{tabular}
\caption{Configuration presets}
\end{table}

\subsection{Pluggable Storage}

While SQLite is the default, the storage interface is abstract:
\begin{verbatim}
class Store(ABC):
    def add(self, memory: Memory) -> str
    def get(self, memory_id: str) -> Memory
    def search(self, query: str) -> List[Memory]
    def update(self, memory: Memory) -> None
\end{verbatim}

Implementations planned/available:
\begin{itemize}
    \item SQLite (included, default)
    \item Supabase (cloud, implemented)
    \item Cloudflare D1 (edge, planned)
\end{itemize}

\section{Experiments}

\subsection{Evaluation Tasks}

We evaluate on four tasks designed to test memory dynamics:

\paragraph{Multi-session continuity} An agent interacts with a user over 10 sessions spanning 7 days. We measure recall of user preferences mentioned in early sessions.

\paragraph{Relevance vs recency} Present the agent with (a) an old but highly relevant memory and (b) a recent but tangentially related memory. Measure which is retrieved.

\paragraph{Forgetting benefits} Compare retrieval quality when irrelevant memories are (a) retained indefinitely vs (b) allowed to decay. Measure signal-to-noise ratio.

\paragraph{Hebbian emergence} Track whether meaningful associations form automatically through co-activation, without manual entity tagging.

\subsection{Baselines}

\begin{itemize}
    \item \textbf{Mem0}: Vector-based with text-embedding-ada-002
    \item \textbf{Zep}: Vector + temporal filtering
    \item \textbf{shodh-memory}: Hebbian + TinyBERT NER
    \item \textbf{Raw context}: No memory system, just LLM context window
\end{itemize}

\subsection{Metrics}

\begin{itemize}
    \item \textbf{Retrieval precision/recall}: Relevant memories retrieved vs missed
    \item \textbf{User preference}: Human evaluation of agent helpfulness
    \item \textbf{Computational cost}: Latency and API calls per retrieval
    \item \textbf{Memory growth}: Storage size over time (with vs without forgetting)
\end{itemize}

\subsection{Results}

[TODO: Run experiments and report results]

Preliminary observations from development testing:
\begin{itemize}
    \item Hebbian links form meaningful associations within 5-10 sessions
    \item Forgetting reduces memory store size by ~30\% while improving retrieval precision
    \item Zero-dependency design adds <1ms latency per retrieval
\end{itemize}

\section{Discussion}

\subsection{When to Use Cognitive Models}

\paragraph{Use NeuromemoryAI when:}
\begin{itemize}
    \item An LLM is already in the pipeline (semantic understanding available)
    \item Long-term agent deployment (dynamics matter over time)
    \item No external API access desired (privacy, offline)
\end{itemize}

\paragraph{Use shodh-memory when:}
\begin{itemize}
    \item Edge deployment required (Raspberry Pi, etc.)
    \item No LLM available (need bundled NER/embeddings)
    \item Standalone operation required
\end{itemize}

\paragraph{Use vector search when:}
\begin{itemize}
    \item Simple applications without long-term state
    \item Existing vector infrastructure
    \item Purely semantic similarity is sufficient
\end{itemize}

\subsection{Limitations}

\begin{itemize}
    \item \textbf{FTS5 vs embeddings}: Keyword search less flexible than semantic similarity. Mitigated by LLM interpretation of results.
    \item \textbf{LLM dependency}: Requires external LLM for semantic understanding. By design, not a limitation.
    \item \textbf{Parameter tuning}: Decay rates, consolidation frequency, Hebbian threshold need tuning per application.
    \item \textbf{Cold start}: New agents have no memory dynamics until patterns accumulate.
\end{itemize}

\subsection{Future Work}

\begin{itemize}
    \item \textbf{Adaptive parameters}: Learn optimal decay/consolidation rates from retrieval success feedback
    \item \textbf{Multi-agent memory}: Shared memory pools with agent-specific views
    \item \textbf{Cloud sync}: Conflict resolution for distributed memory updates
    \item \textbf{Framework integration}: LangChain, CrewAI, AutoGen adapters
\end{itemize}

\section{Conclusion}

Memory for AI agents should not be reduced to vector similarity search. By implementing established cognitive science models---ACT-R activation for principled retrieval, Hebbian learning for emergent associations, Memory Chain consolidation for dual-trace dynamics, and Ebbinghaus forgetting for adaptive decay---we create memory systems that behave more like biological memory: strengthening with use, fading without it, and forming emergent structure through experience.

\textsc{NeuromemoryAI} demonstrates that these models can be implemented efficiently (zero dependencies, ~500 lines of core code) while providing principled alternatives to engineering heuristics. The key insight is the division of labor: LLMs provide semantic understanding; cognitive models provide memory dynamics. Together, they enable agents that truly remember.

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}
